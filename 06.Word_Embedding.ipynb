{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "**Non Contextual Word embedding** (или векторное представление слов) - техника в NLP, когда с помощью unsupervised (чаще всего) алгоритма слову или фразе из словаря сопоставляется вещественный вектор фиксированной размерности. Этот вектор (чаще всего) тем или иным образом характеризует семантику слова. Вектор может зависеть от контекста слова (и быть чувствительным к омонимии), а может и не зависеть. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "[Word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - один из первых методов векторного представления слов,  ставших популярным. \n",
    "\n",
    "Два подхода:\n",
    "- Continues Bag of Words (CBOW)\n",
    "- Skip gram\n",
    "\n",
    "![Источник - https://arxiv.org/pdf/1301.3781.pdf ](img/w2v.png)\n",
    "\n",
    "Вероятность слова в контексте можно интерпретировать как:\n",
    "\n",
    "$$p(w_o|w_I) = \\frac{\\exp(v′_{w_O}^T \\cdot v_{w_I})}{\\sum_{w=1}^{W}\\exp(v′_{w_i}^T \\cdot v_{w_I})}$$\n",
    "\n",
    "Для оптимизации используется или negative sampling или hierarchical softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) - библиотека от Facebook.\n",
    "- большое количество предтренированныйх моделей для разных языков\n",
    "- может сопоставлять вектора для слов вне словаря\n",
    "\n",
    "Принцип работы - слово делится на N-граммы. Каждой N-грамме сопоставляется вектор, для получения векторного представления всего слова вектора N-грамм суммируются. \n",
    "\n",
    "**привет** $\\rightarrow$ **&lt;пр**, **при**, **рив**, **иве**, **вет**, **ет&gt;**. \n",
    "\n",
    "Дальше идея таже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) - имплементация через минимизацию функционала:\n",
    "\n",
    "$$J = \\sum_{i,j=1}^V f(X_{ij}) (w_i \\cdot \\tilde{w_j} + b_i + \\tilde{b_j} - \\log X_{ij})$$\n",
    "\n",
    "где, $X_{ij}$ - матрица взаимовстречаемости слов (сколько раз слово $i$ имело в контексте слова $j$). \n",
    "\n",
    "$f(x)= \\begin{cases}\n",
    "    (\\frac{x}{x_{max}})^\\alpha, & \\text{если }  x<x_{max}.\\\\\n",
    "    1, & \\text{иначе}.\n",
    "  \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения Word2Vec с помощью [gensim](https://github.com/RaRe-Technologies/gensim)\n",
    "\n",
    "\n",
    "Каждая строчка в архиве - тройка\n",
    "> **category** \\\\t **headline** \\\\t **text**\n",
    "\n",
    "Где:\n",
    "- category - категоря новости\n",
    "- headline - заголовок\n",
    "- text - текст новости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "# Чтение файла данных\n",
    "def read_texts(fn: str=\"data/news.txt.gz\") -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))\n",
    "                    \n",
    "# Разбиение текста на слова                 \n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "# Текст без знаков припенания (нужен для gensim)\n",
    "def normalize_text(text: str) -> str:\n",
    "    return ' '.join(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('интерфакс', 0.8428274989128113),\n",
       " ('тасс', 0.8292704224586487),\n",
       " ('источник', 0.8029454350471497),\n",
       " ('итар', 0.7665677070617676),\n",
       " ('reuters', 0.7380065321922302),\n",
       " ('риа', 0.7300341725349426),\n",
       " ('агентство', 0.7267807126045227),\n",
       " ('коммерсантъ', 0.7247551679611206),\n",
       " ('bloomberg', 0.7195194363594055),\n",
       " ('прайм', 0.7191484570503235)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Обучение word2vec\n",
    "# каждый текст - набор слов через пробел\n",
    "sentences = [tokenize_text(text.text) for text in read_texts()]\n",
    "\n",
    "# обучаем w2v\n",
    "w2v = Word2Vec(sentences)\n",
    "\n",
    "# сохраняем модель\n",
    "w2v.wv.save_word2vec_format('w2v_vectors.bin')\n",
    "# пример\n",
    "w2v.wv.most_similar(\"новости\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
