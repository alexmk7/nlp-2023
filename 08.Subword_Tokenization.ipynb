{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword tokenization\n",
    "\n",
    " - основная идея: разделять текст не по словам, а по камим общим общим подсловам. Например `мирный` $\\rightarrow$ [`мир`, `##н`, `##ый`]\n",
    " - можно ограничивать размер словаря\n",
    " - есть техники регуляризации\n",
    " - работает для агглютинативных языков \n",
    " - можно обрабатывать незнакомые слова\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "\n",
    "Процесс начинается со словаря, который состоит из букв. Затем на каждом шаге два элемента объеденяются такием образом, чтобы максимальным образом увеличить вероятность текста с точки зрения языковой модели модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "Похоже на WordPiece, но считается количество повторений двух соседних элементов в словаре на каждом шаге. Элементы словаря с максимальной частотой сливаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto as tqdm\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable\n",
    "\n",
    "# Чтение файла данных\n",
    "def read_texts(fn: str=\"data/news.txt.gz\") -> Iterable[str]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split(\"\\t\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=10000)\n",
    "\n",
    "tokenizer.train_from_iterator(read_texts(), trainer=trainer)\n",
    "tokenizer.save(\"data/news_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/news_tokenizer.json\")\n",
    "res = tokenizer.encode(\"первому корове привет миру прививка\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(length=50)\n",
    "\n",
    "import torch\n",
    "res = tokenizer.encode_batch([\"первому корове привет миру прививка\", \"хорошая погода\"])\n",
    "torch.tensor([x.ids for x in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(10000, 100)\n",
    "emb(torch.tensor([x.ids for x in tokenizer.encode_batch([\"хорошая погода\", \"привет мир\"])])).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
